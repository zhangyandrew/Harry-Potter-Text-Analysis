---
title: "Text Analysis of Correlaid"
author: "Qianhui Rong"
date: "11/3/2018"
output: pdf_document
---

```{r setup, include=FALSE,warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Seperate Analysis on Each Article**
## P-Value Article 
We want to analyze the passage from https://correlaid.org/blog/posts/understand-p-values. 
```{r}
library(tidytext)
library(dplyr)
library(stringr)
library(ggplot2)
library(tidyr)

correlaid_txt <- read.delim("correlaid_pvalue.txt")
correlaid_txt <- data.frame(lapply(correlaid_txt, as.character), stringsAsFactors=FALSE)
colnames(correlaid_txt) <- c("text")
correlaid_txt %>% unnest_tokens(output = word,input = text) -> token_correlaid

data("stop_words")
token_correlaid %>%
  anti_join(stop_words) -> tidy_correlaid

tidy_correlaid %>% 
  count(word,sort=TRUE)

tidy_correlaid %>% 
  count(word,sort=TRUE) %>%
  top_n(20) %>% 
  mutate(word=reorder(word,n)) %>%  #reorder 
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()
```
After eliminating the stop words in the article, we order the words appeared in the passage by frequency and we made a ggplot to show the 20 most frequent words appear in the article.

```{r}
sentiment_correlaid <- tidy_correlaid %>%  #With Bing 
  inner_join(get_sentiments("bing"))%>% 
  mutate(method = "Bing")
sentiment_correlaid %>% 
  count(word,sort=TRUE) %>%
  top_n(20) %>% 
  mutate(word=reorder(word,n)) %>% 
  ggplot(aes(word, n)) + 
  geom_col() +
  xlab(NULL) +
  coord_flip()
```
In order to get an odea of the passage's sentiment on P-Value, we applied Bing sentiment package, and made a ggplot of the top 20 sentimental words in the article. The first one "significant" is about 3 times more frequent than the second word in order. That should be due to the term "statistically significant".
Then we want to compare the results from the other two packages of sentimenal words: AFINN and NRC.
```{r}
sentiment_correlaid_AF <- tidy_correlaid %>% 
  inner_join(get_sentiments("afinn"))%>% 
  mutate(method = "AFINN")
sentiment_correlaid_NRC <- tidy_correlaid %>% 
  inner_join(get_sentiments("nrc"))%>% 
  mutate(method = "NRC")

bind_rows(sentiment_correlaid,
          sentiment_correlaid_NRC) %>%
  mutate(Count=n()) -> three_pack
ggplot(aes(sentiment, Count,fill = method),data=three_pack) +
  geom_col(show.legend = FALSE)+
  facet_wrap(~method, ncol = 1, scales = "free_y")+
  theme(axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5))+
  coord_flip()+
  ggtitle("Plot of NRC and Bing")

#Because AFINN's results are in numerical continuous scale, so we draw a seperate plot for it.

sentiment_correlaid_AF %>% 
  ggplot()+
  geom_histogram(aes(x=score,fill=word),stat="bin",show.legend = TRUE)+
  theme(legend.position="bottom")+
  scale_alpha_discrete(breaks=c(-3,-2,-1,0,1,2,3))+
  ggtitle("Plot of AFINN")
```

We want to also see the wordcloud.
```{r}
library(wordcloud)
tidy_correlaid %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))

library(reshape2)
tidy_correlaid %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("black", "red"),
                   max.words = 100)
```


## From the Data to the Story Article 
We want to analyze the passage from https://correlaid.org/blog/posts/journocode-workflow. 
```{r}
library(tidytext)
library(dplyr)
library(stringr)
library(ggplot2)

correlaid_txt2 <- read.delim("correlaid_fromdatatostory.txt")
correlaid_txt2 <- data.frame(lapply(correlaid_txt2, as.character), stringsAsFactors=FALSE)
colnames(correlaid_txt2) <- c("text")
correlaid_txt2 %>% unnest_tokens(output = word,input = text) -> token_correlaid2

data("stop_words")
token_correlaid2 %>%
  anti_join(stop_words) -> tidy_correlaid2

tidy_correlaid2 %>% 
  count(word,sort=TRUE)

tidy_correlaid2 %>% 
  count(word,sort=TRUE) %>%
  top_n(20) %>% 
  mutate(word=reorder(word,n)) %>%  #reorder 
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()
```
After eliminating the stop words in the article, we order the words appeared in the passage by frequency and we made a ggplot to show the 20 most frequent words appear in the article.

```{r}
sentiment_correlaid2 <- tidy_correlaid2 %>%  #With Bing 
  inner_join(get_sentiments("bing"))%>% 
  mutate(method = "Bing")
sentiment_correlaid2 %>% 
  count(word,sort=TRUE) %>%
  top_n(20) %>% 
  mutate(word=reorder(word,n)) %>% 
  ggplot(aes(word, n)) + 
  geom_col() +
  xlab(NULL) +
  coord_flip()
```
In order to get an odea of the passage's sentiment on this article, we applied Bing sentiment package, and made a ggplot of the top 20 sentimental words in the article. The first three are "plot", "excel" and "tidy", which is reasonable because this is a tutorial of R.
Then we want to compare the results from the other two packages of sentimenal words: AFINN and NRC.

```{r}
sentiment_correlaid_AF2 <- tidy_correlaid2 %>% 
  inner_join(get_sentiments("afinn"))%>% 
  mutate(method = "AFINN")
sentiment_correlaid_NRC2 <- tidy_correlaid2 %>% 
  inner_join(get_sentiments("nrc"))%>% 
  mutate(method = "NRC")

bind_rows(sentiment_correlaid2,
          sentiment_correlaid_NRC2) %>%
  mutate(Count=n()) -> three_pack2
ggplot(aes(sentiment, Count,fill = method),data=three_pack2) +
  geom_col(show.legend = FALSE)+
  facet_wrap(~method, ncol = 1, scales = "free_y")+
  theme(axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5))+
  coord_flip()+
  ggtitle("Plot of NRC and Bing")

#Because AFINN's results are in numerical continuous scale, so we draw a seperate plot for it.

sentiment_correlaid_AF2 %>% 
  ggplot()+
  geom_histogram(aes(x=score,fill=word),stat="bin",show.legend = TRUE)+
  theme(legend.position="bottom")+
  scale_alpha_discrete(breaks=c(-3,-2,-1,0,1,2,3))+
  ggtitle("Plot of AFINN")
```

We want to also see the wordcloud.
```{r}
library(wordcloud)
tidy_correlaid2 %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))

library(reshape2)
tidy_correlaid2 %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("black", "red"),
                   max.words = 100)
```


**Combined Analysis on Two Articles**
To find important words for the context by decreasing the weight for commonly used words, we apply bind_tf_idf function for these two article.
```{r}
tidy_correlaid %>% group_by(word) %>% 
  mutate(n=n()) %>% 
  mutate(article="P-value") %>% 
  arrange(n)-> correlaid_words
tidy_correlaid2 %>% group_by(word) %>% 
  mutate(n=n()) %>% 
  mutate(article="Data-Story") %>% 
  arrange(n) -> correlaid_words2
total_words <- rbind(correlaid_words,correlaid_words2) 
total_words %>% group_by(article) %>% 
  mutate(total=sum(n)) %>% 
  mutate(rank=row_number(), `term frequency`= n/total) %>% 
  arrange(desc(`term frequency`))-> all_words
#all_words has all information we need to do tf_idf analysis.

ggplot(all_words, aes(n/total, fill = article)) +
  geom_histogram(show.legend = FALSE) +
  facet_wrap(~article, nrow = 2, scales = "free_y")+
  ggtitle("Frequency VS Count")

correlaid_words <- correlaid_words %>% mutate(proportion=n/sum(n))
correlaid_words2 <- correlaid_words2 %>% mutate(proportion=n/sum(n))

```
We can see that the tails are not so long and these two article exhibit similar distribution. Their peaks are at similar points.
```{r}
ggplot(all_words,aes(rank, `term frequency`, color = article)) + geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()+
  ggtitle("Rand VS Term Frequency")
```
The result is totally opposite to the Zipf's Law, which states that a word appears is inversely proportional to its rank.

Then we apply bind_tf_idf function to find the important words for the content of each document by decreasing the weight for commonly used words and increasing the weight for words that not used very much. 
```{r}
all_words <- all_words %>% bind_tf_idf(word, article, n)
all_words
```

**N-grams and Correlations**
We want to check the words as bigrams from now on.
```{r}
correlaid_bigrams <- correlaid_txt %>% unnest_tokens(bigram, text, token = "ngrams", n = 2) #for p-value article
correlaid_bigrams2 <- correlaid_txt2 %>% unnest_tokens(bigram, text, token = "ngrams", n = 2) #for data-story article

#Seperate the bigrams into two words
bigrams_separated <- correlaid_bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ")
bigrams_separated2 <- correlaid_bigrams2 %>%
separate(bigram, c("word1", "word2"), sep = " ")
#Eliminate stop words
bigrams_filtered <- bigrams_separated %>% filter(!word1 %in% stop_words$word) %>% filter(!word2 %in% stop_words$word)
bigrams_filtered2 <- bigrams_separated2 %>% filter(!word1 %in% stop_words$word) %>% filter(!word2 %in% stop_words$word)
#Then unite them into bigrams
bigrams_united <- bigrams_filtered %>% unite(bigram, word1, word2, sep = " ")
bigrams_united2 <- bigrams_filtered2 %>% unite(bigram, word1, word2, sep = " ")
bigrams_united %>% mutate(article="P-Value") -> bigrams_united
bigrams_united2 %>% mutate(article="Data-Story") -> bigrams_united2
```

Then we apply bind_tf_idf function to find the important bigrams.
```{r}
total_bigrams <- rbind(bigrams_united,bigrams_united2)
total_bigrams %>% 
  mutate(n=n()) %>% 
  bind_tf_idf(bigram, article, n)  
```

Using bigrams to do sentiments analysis.
If we do seperate analysis on both article about "not" words.
```{r}
AFINN <- get_sentiments("afinn")
not_words <- bigrams_separated %>%
filter(word1 == "not") %>%
inner_join(AFINN, by = c(word2 = "word")) %>% count(word2, score, sort = TRUE) %>% ungroup()

not_words %>%
  mutate(contribution = n * score) %>% 
  arrange(desc(abs(contribution))) 
```
In this P-Value article, only one word is follwed by "not".

```{r}
not_words2 <- bigrams_separated2 %>%
  filter(word1 == "not") %>%
  inner_join(AFINN, by = c(word2 = "word")) %>% 
  count(word2, score, sort = TRUE) %>% ungroup()

not_words2 %>%
  mutate(contribution = n * score) %>% 
  arrange(desc(abs(contribution))) 
```
And in this Data to Story article, on word is followed by "not".

**Network of Bigrams**
```{r}
library(igraph)
library(ggraph)

#Network for P-Value Article
bigrams_separated <- 
  correlaid_bigrams %>%
  mutate(n=n()) %>% 
  separate(bigram, c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word2 %in% stop_words$word)
bigrams_separated %>% graph_from_data_frame() -> bigram_graph
set.seed(2018)
ggraph(bigram_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)+
  coord_fixed(0.8)+
  ggtile("Network Plot for P-Value Article")

#Network for Data to Story Article
bigrams_separated2 <- 
  correlaid_bigrams2 %>%
  mutate(n=n()) %>% 
  separate(bigram, c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word2 %in% stop_words$word)
bigrams_separated2 %>% graph_from_data_frame() -> bigram_graph2
set.seed(2018)
ggraph(bigram_graph2, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)+
  coord_fixed(0.8)+
  ggtile("Network Plot for Data to Story Article")
```

